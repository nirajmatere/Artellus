{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. installation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install scikit-learn\n",
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources (run this only once)\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_text = \"Deep learning is a branch of artificial intelligence that involves training neural networks on large datasets. These networks can learn complex patterns and representations, enabling them to perform tasks such as image recognition, natural language processing, and speech synthesis. The advancements in deep learning have led to breakthroughs in various fields, including healthcare, finance, and autonomous vehicles.\"\n",
    "\n",
    "corpus_text = \"Mount Everest, the highest peak in the world, is located in the Himalayas. The stunning landscape offers breathtaking views of snow-capped mountains and serene valleys. Visitors often describe the experience as awe-inspiring. The local flora and fauna, including rare species, add to the unique charm of the region. Exploring the Everest Base Camp is a popular adventure for trekking enthusiasts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = preprocess_text(corpus_text)\n",
    "# print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Named Entity Recognition (NER) and Technical Keywords Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_technical_keywords(text, num_keywords=5):\n",
    "    words = word_tokenize(text)\n",
    "    tagged_words = pos_tag(words)\n",
    "    \n",
    "    # Extract nouns and technical terms\n",
    "    technical_keywords = [word for word, pos in tagged_words if pos.startswith('N') or pos.startswith('JJ')]\n",
    "\n",
    "    # Limit to num_keywords\n",
    "    return technical_keywords[:num_keywords]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def extract_entities(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    technical_keywords = extract_technical_keywords(preprocessed_text)\n",
    "    # entities.append(technical_keywords)    \n",
    "    for tech_key in technical_keywords:\n",
    "        entities.append(tech_key)\n",
    "    return entities\n",
    "\n",
    "# Example usage:\n",
    "entities = extract_entities(preprocessed_text)\n",
    "# print(entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def extract_keywords(text):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    keywords = [feature_names[i] for i in tfidf_matrix.sum(axis=0).argsort()[0, ::-1][:5]]  # Extract top 5 keywords\n",
    "    return keywords\n",
    "\n",
    "# Example usage:\n",
    "keywords = extract_keywords(preprocessed_text)\n",
    "# print(keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = \"positive\" if blob.sentiment.polarity > 0 else \"negative\" if blob.sentiment.polarity < 0 else \"neutral\"\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "sentiment = analyze_sentiment(corpus_text)\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Constructing the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(entities, keywords, sentiment):\n",
    "    # Convert entities and keywords to strings\n",
    "    entities_str = ', '.join(map(str, entities))\n",
    "    keywords_str = ', '.join(map(str, keywords[0][0]))\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"Generate an image featuring {entities_str} with a {sentiment} mood, emphasizing {keywords_str}.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate an image featuring Mount Everest, Himalayas, Exploring Everest Base Camp, Mount, Everest, highest, peak, world with a positive mood, emphasizing everest, world, including, himalayas, highest, flora, fauna, exploring, experience, enthusiasts, visitors, describe, charm, camp, breathtaking, base, aweinspiring, adventure, landscape, local, located, mount, views, valleys, unique, trekking, stunning, species, snowcapped, serene, region, rare, popular, peak, often, offers, mountains, add.\n"
     ]
    }
   ],
   "source": [
    "prompt = construct_prompt(entities, keywords, sentiment)\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
